{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP AUTOENCODING GAUSSIAN MIXTURE MODEL FOR UNSUPERVISED ANOMALY DETECTION [link](https://openreview.net/pdf?id=BJJLHbb0-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle as pl\n",
    "import numpy as np\n",
    "from barbar import Bar\n",
    "\n",
    "#from train import TrainerDAGMM\n",
    "#from test import eval\n",
    "#from preprocess import get_KDDCup99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    num_epochs=200\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=1024\n",
    "    latent_dim=1\n",
    "    n_gmm=4\n",
    "    lambda_energy=0.1\n",
    "    lambda_cov=0.005\n",
    "    \n",
    "    \n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDDCupData:\n",
    "    def __init__(self, data_dir, mode):\n",
    "        \"\"\"Loading the data for train and test.\"\"\"\n",
    "        data = np.load(data_dir, allow_pickle=True)\n",
    "\n",
    "        labels = data[\"kdd\"][:,-1]\n",
    "        features = data[\"kdd\"][:,:-1]\n",
    "        \n",
    "        #'attack'을 정상데이터로 지정\n",
    "        normal_data = features[labels==0] \n",
    "        normal_labels = labels[labels==0]\n",
    "\n",
    "        n_train = int(normal_data.shape[0]*0.5)\n",
    "        ixs = np.arange(normal_data.shape[0])\n",
    "        np.random.shuffle(ixs)\n",
    "        normal_data_test = normal_data[ixs[n_train:]]\n",
    "        normal_labels_test = normal_labels[ixs[n_train:]]\n",
    "\n",
    "        # train, test 데이터셋으로 split\n",
    "        if mode == 'train':\n",
    "            self.x = normal_data[ixs[:n_train]]\n",
    "            self.y = normal_labels[ixs[:n_train]]\n",
    "        elif mode == 'test':\n",
    "            anomalous_data = features[labels==1]\n",
    "            anomalous_labels = labels[labels==1]\n",
    "        # 정상과 anomaly 데이터로 분리\n",
    "            self.x = np.concatenate((anomalous_data, normal_data_test), axis=0)\n",
    "            self.y = np.concatenate((anomalous_labels, normal_labels_test), axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        # object 데이터셋에 있는 이미지 수\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 데이터셋으로부터 sample return\n",
    "        return np.float32(self.x[index]), np.float32(self.y[index])\n",
    "\n",
    "\n",
    "\n",
    "def get_KDDCup99(args, data_dir='./data/kdd_cup.npz'):\n",
    "    \"\"\"Returning train and test dataloaders.\"\"\"\n",
    "    train = KDDCupData(data_dir, 'train')\n",
    "    dataloader_train = DataLoader(train, batch_size=args.batch_size, \n",
    "                              shuffle=True, num_workers=0)\n",
    "    \n",
    "    test = KDDCupData(data_dir, 'test')\n",
    "    dataloader_test = DataLoader(test, batch_size=args.batch_size, \n",
    "                              shuffle=False, num_workers=0)\n",
    "    return dataloader_train, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './data/kdd_cup.npz'\n",
    "data = np.load(data_dir, allow_pickle=True)\n",
    "# for i in data:\n",
    "#     print(i)\n",
    "data['kdd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494021, 119)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['kdd'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마지막 열을 라벨로 지정\n",
    "labels = data[\"kdd\"][:,-1]\n",
    "# 나머지 부분들을 feature로 지정\n",
    "features = data[\"kdd\"][:,:-1]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'attack'을 정상데이터로 지정\n",
    "normal_data = features[labels==0] \n",
    "normal_labels = labels[labels==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(normal_data.shape[0]*0.5)\n",
    "ixs = np.arange(normal_data.shape[0])\n",
    "np.random.shuffle(ixs)\n",
    "normal_data_test = normal_data[ixs[n_train:]]\n",
    "normal_labels_test = normal_labels[ixs[n_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198372, 118)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = KDDCupData(data_dir, 'train')\n",
    "dataloader_train = DataLoader(train, batch_size=args.batch_size, \n",
    "                            shuffle=True, num_workers=0)\n",
    "\n",
    "test = KDDCupData(data_dir, 'test')\n",
    "dataloader_test = DataLoader(test, batch_size=args.batch_size, \n",
    "                            shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_KDDCup99(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/DAGMM/IMG1.png\" height = \"60%\" width = \"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAGMM(nn.Module):\n",
    "    def __init__(self, n_gmm=2, z_dim=1):\n",
    "        \"\"\"Network for DAGMM (KDDCup99)\"\"\"\n",
    "        super(DAGMM, self).__init__()\n",
    "        \n",
    "        # Encoder 네트워크 : 118차원에서 최종 z_dim으로 축소\n",
    "        self.fc1 = nn.Linear(118, 60)\n",
    "        self.fc2 = nn.Linear(60, 30)\n",
    "        self.fc3 = nn.Linear(30, 10)\n",
    "        self.fc4 = nn.Linear(10, z_dim)\n",
    "\n",
    "        # Decoder 네트워크 : z_dim에서 최종 118차원으로 확대\n",
    "        self.fc5 = nn.Linear(z_dim, 10)\n",
    "        self.fc6 = nn.Linear(10, 30)\n",
    "        self.fc7 = nn.Linear(30, 60)\n",
    "        self.fc8 = nn.Linear(60, 118)\n",
    "\n",
    "        # Estimation 네트워크 : GMM으로 밀도 추정\n",
    "        # z_dim + 2 = z_c + z_r (복원 데이터의 오차에서 파생된 변수들)\n",
    "        # z_r = f(x, x')\n",
    "        # z = [z_c, z_r]\n",
    "        self.fc9 = nn.Linear(z_dim+2, 10)\n",
    "        # parameter 추정\n",
    "        self.fc10 = nn.Linear(10, n_gmm)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = torch.tanh(self.fc1(x))\n",
    "        h = torch.tanh(self.fc2(h))\n",
    "        h = torch.tanh(self.fc3(h))\n",
    "        return self.fc4(h)\n",
    "\n",
    "    def decode(self, x):\n",
    "        h = torch.tanh(self.fc5(x))\n",
    "        h = torch.tanh(self.fc6(h))\n",
    "        h = torch.tanh(self.fc7(h))\n",
    "        return self.fc8(h)\n",
    "    \n",
    "    def estimate(self, z):\n",
    "        h = F.dropout(torch.tanh(self.fc9(z)), 0.5)\n",
    "        return F.softmax(self.fc10(h), dim=1)\n",
    "    \n",
    "    def compute_reconstruction(self, x, x_hat):\n",
    "        relative_euclidean_distance = (x-x_hat).norm(2, dim=1) / x.norm(2, dim=1)\n",
    "        cosine_similarity = F.cosine_similarity(x, x_hat, dim=1)\n",
    "        return relative_euclidean_distance, cosine_similarity\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_c = self.encode(x)\n",
    "        x_hat = self.decode(z_c)\n",
    "        # reconstruction error 구하기\n",
    "        rec_1, rec_2 = self.compute_reconstruction(x, x_hat)\n",
    "        z = torch.cat([z_c, rec_1.unsqueeze(-1), rec_2.unsqueeze(-1)], dim=1)\n",
    "        gamma = self.estimate(z)\n",
    "        return z_c, x_hat, z, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    \n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1 and classname != 'Conv':\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        torch.nn.init.normal_(m.bias.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        torch.nn.init.normal_(m.bias.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.01)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **sample 에너지**\n",
    "\n",
    "    * 각 sample의 mixture membership을 예측하기 위해 mln 이요\n",
    "    * 높은 에너지를 가지면 threshold를 기준으로 나누어 anomaly로 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/DAGMM/IMG2.png\" height = \"60%\" width = \"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GMM parameters**\n",
    "\n",
    "    * _phi_ : mixture-component distribution\n",
    "    * _mu_ : mixture means\n",
    "    * _sigma_ : mixture covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/DAGMM/IMG3.png\" height = \"60%\" width = \"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ComputeLoss:\n",
    "    def __init__(self, model, lambda_energy, lambda_cov, device, n_gmm):\n",
    "        self.model = model\n",
    "        self.lambda_energy = lambda_energy\n",
    "        self.lambda_cov = lambda_cov\n",
    "        self.device = device\n",
    "        self.n_gmm = n_gmm\n",
    "\n",
    "    def forward(self, x, x_hat, z, gamma):\n",
    "        ## DAGMM을 위한 loss function\n",
    "        reconst_loss = torch.mean((x-x_hat).pow(2))\n",
    "\n",
    "        sample_energy, cov_diag = self.compute_energy(z, gamma)\n",
    "\n",
    "        loss = reconst_loss + self.lambda_energy * sample_energy + self.lambda_cov * cov_diag\n",
    "        return Variable(loss, requires_grad=True)\n",
    "\n",
    "    def compute_energy(self, z, gamma, phi=None, mu=None, cov=None, sample_mean=True):\n",
    "        ## sample energy function 계산\n",
    "        if (phi is None) or (mu is None) or (cov is None):\n",
    "            phi, mu, cov = self.compute_params(z, gamma)\n",
    "\n",
    "        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n",
    "\n",
    "        eps = 1e-12\n",
    "        cov_inverse = []\n",
    "        det_cov = []\n",
    "        cov_diag = 0\n",
    "\n",
    "        for k in range(self.n_gmm):\n",
    "            cov_k = cov[k] + (torch.eye(cov[k].size(-1))*eps).to(self.device)\n",
    "            cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n",
    "            det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n",
    "            cov_diag += torch.sum(1 / cov_k.diag())\n",
    "        \n",
    "        cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "        det_cov = torch.cat(det_cov).to(self.device)\n",
    "\n",
    "        E_z = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n",
    "        E_z = torch.exp(E_z)\n",
    "        E_z = -torch.log(torch.sum(phi.unsqueeze(0)*E_z / (torch.sqrt(det_cov)).unsqueeze(0), dim=1) + eps)\n",
    "        if sample_mean==True:\n",
    "            E_z = torch.mean(E_z)            \n",
    "        return E_z, cov_diag\n",
    "\n",
    "    def compute_params(self, z, gamma):\n",
    "        ## sample energy 함수를 위해 파라미터(phi, mu, gamma) 계산\n",
    "        # K: number of Gaussian mixture components\n",
    "        # N: Number of samples\n",
    "        # D: Latent dimension\n",
    "        # z = NxD\n",
    "        # gamma = NxK\n",
    "\n",
    "        #phi = D\n",
    "        phi = torch.sum(gamma, dim=0)/gamma.size(0) \n",
    "\n",
    "        #mu = KxD\n",
    "        mu = torch.sum(z.unsqueeze(1) * gamma.unsqueeze(-1), dim=0)\n",
    "        mu /= torch.sum(gamma, dim=0).unsqueeze(-1)\n",
    "\n",
    "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "        z_mu_z_mu_t = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "        \n",
    "        #cov = K x D x D\n",
    "        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_z_mu_t, dim=0)\n",
    "        cov /= torch.sum(gamma, dim=0).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        return phi, mu, cov\n",
    "    \n",
    "# 행렬 A가 대칭행렬일 때 하삼각행렬과 상삼각행렬로 분해 가능\n",
    "class Cholesky(torch.autograd.Function):\n",
    "    def forward(ctx, a):\n",
    "        l = torch.cholesky(a, False)\n",
    "        ctx.save_for_backward(l)\n",
    "        return l\n",
    "    def backward(ctx, grad_output):\n",
    "        l, = ctx.saved_variables\n",
    "        linv = l.inverse()\n",
    "        inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n",
    "            1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n",
    "        s = torch.mm(linv.t(), torch.mm(inner, linv))\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerDAGMM:\n",
    "    # DAGMM을 위한 trainer class\n",
    "    def __init__(self, args, data, device):\n",
    "        self.args = args\n",
    "        self.train_loader, self.test_loader = data\n",
    "        self.device = device\n",
    "\n",
    "    def train(self):\n",
    "        # DAGMM 모델 train\n",
    "        self.model = DAGMM(self.args.n_gmm, self.args.latent_dim).to(self.device)\n",
    "        self.model.apply(weights_init_normal)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "\n",
    "        self.compute = ComputeLoss(self.model, self.args.lambda_energy, self.args.lambda_cov, \n",
    "                                   self.device, self.args.n_gmm)\n",
    "        self.model.train()\n",
    "        for epoch in range(self.args.num_epochs):\n",
    "            total_loss = 0\n",
    "            for x, _ in Bar(self.train_loader):\n",
    "                x = x.float().to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                _, x_hat, z, gamma = self.model(x)\n",
    "\n",
    "                loss = self.compute.forward(x, x_hat, z, gamma)\n",
    "                loss.backward(retain_graph=True)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            print('Training DAGMM... Epoch: {}, Loss: {:.3f}'.format(\n",
    "                   epoch, total_loss/len(self.train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40960/198371: [======>.........................] - ETA 2.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunwookim/.local/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
      "L = torch.cholesky(A)\n",
      "should be replaced with\n",
      "L = torch.linalg.cholesky(A)\n",
      "and\n",
      "U = torch.cholesky(A, upper=True)\n",
      "should be replaced with\n",
      "U = torch.linalg.cholesky(A).transpose(-2, -1).conj().\n",
      "This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1285.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 0, Loss: 22276149.433\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 1, Loss: 22309584.562\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 2, Loss: 22301232.959\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 3, Loss: 22269940.495\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 4, Loss: 22280015.062\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 5, Loss: 22293858.165\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 6, Loss: 22330393.464\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 7, Loss: 22274242.588\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 8, Loss: 22315220.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 9, Loss: 22300047.588\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 10, Loss: 22331002.649\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 11, Loss: 22315675.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 12, Loss: 22283575.423\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 13, Loss: 22318095.021\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 14, Loss: 22297323.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 15, Loss: 22245997.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 16, Loss: 22262768.412\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 17, Loss: 22278221.299\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 18, Loss: 22277474.876\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 19, Loss: 22296695.062\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 20, Loss: 22279110.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 21, Loss: 22329411.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 22, Loss: 22261920.722\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 23, Loss: 22301406.402\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 24, Loss: 22276933.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 25, Loss: 22291088.062\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 26, Loss: 22342915.320\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 27, Loss: 22292371.479\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 28, Loss: 22330089.041\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 29, Loss: 22278119.526\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 30, Loss: 22334475.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 31, Loss: 22295502.278\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 32, Loss: 22272363.103\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 33, Loss: 22271010.897\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 34, Loss: 22281221.835\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 35, Loss: 22314603.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 36, Loss: 22266161.938\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 37, Loss: 22270164.196\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 38, Loss: 22296404.454\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 39, Loss: 22285588.959\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 40, Loss: 22273657.670\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 41, Loss: 22269197.454\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 42, Loss: 22270227.835\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 43, Loss: 22305577.649\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 44, Loss: 22291136.371\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 45, Loss: 22304254.784\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 46, Loss: 22326817.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 47, Loss: 22276272.474\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 48, Loss: 22301268.340\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 49, Loss: 22329379.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 50, Loss: 22258477.598\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 51, Loss: 22282746.495\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 52, Loss: 22303790.000\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 53, Loss: 22273691.474\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 54, Loss: 22309005.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 55, Loss: 22307783.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 56, Loss: 22293114.732\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 57, Loss: 22275236.186\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 58, Loss: 22254881.794\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 59, Loss: 22259070.588\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 60, Loss: 22332640.964\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 61, Loss: 22266133.608\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 62, Loss: 22292340.814\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 63, Loss: 22315483.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 64, Loss: 22281447.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 65, Loss: 22304888.454\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 66, Loss: 22285044.247\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 67, Loss: 22303483.340\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 68, Loss: 22313199.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 69, Loss: 22307235.247\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 70, Loss: 22289254.608\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 71, Loss: 22325508.979\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 72, Loss: 22300013.113\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 73, Loss: 22311093.021\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 74, Loss: 22258503.505\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 75, Loss: 22325295.515\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 76, Loss: 22260054.165\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 77, Loss: 22292941.866\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 78, Loss: 22292074.258\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 79, Loss: 22312863.639\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 80, Loss: 22299893.495\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 81, Loss: 22293843.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 82, Loss: 22312627.959\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 83, Loss: 22293020.588\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 84, Loss: 22316633.515\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 85, Loss: 22320115.433\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 86, Loss: 22287079.402\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 87, Loss: 22291502.278\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 88, Loss: 22290651.856\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 89, Loss: 22324061.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 90, Loss: 22295858.897\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 91, Loss: 22306053.742\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 92, Loss: 22269792.072\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 93, Loss: 22279652.804\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 94, Loss: 22292011.227\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 95, Loss: 22275945.732\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 96, Loss: 22296978.897\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 97, Loss: 22310327.103\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 98, Loss: 22312498.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 99, Loss: 22325533.433\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 100, Loss: 22293564.918\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 101, Loss: 22302270.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 102, Loss: 22320097.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 103, Loss: 22276579.804\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 104, Loss: 22313126.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 105, Loss: 22295086.526\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 106, Loss: 22269812.526\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 107, Loss: 22318972.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 108, Loss: 22313495.371\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 109, Loss: 22259418.938\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 110, Loss: 22306282.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 111, Loss: 22283920.423\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 112, Loss: 22273534.144\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 113, Loss: 22286134.974\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 114, Loss: 22330492.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 115, Loss: 22295082.588\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 116, Loss: 22281266.443\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 117, Loss: 22321190.876\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 118, Loss: 22301082.567\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 119, Loss: 22294017.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 120, Loss: 22280989.505\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 121, Loss: 22273705.474\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 122, Loss: 22299915.835\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 123, Loss: 22316170.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 124, Loss: 22318636.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 125, Loss: 22283970.351\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 126, Loss: 22259752.227\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 127, Loss: 22301542.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 128, Loss: 22305543.814\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 129, Loss: 22325602.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 130, Loss: 22283254.093\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 131, Loss: 22295067.103\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 132, Loss: 22272156.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 133, Loss: 22323973.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 134, Loss: 22313434.603\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 135, Loss: 22307778.072\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 136, Loss: 22274897.515\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 137, Loss: 22282434.227\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 138, Loss: 22284513.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 139, Loss: 22265047.392\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 140, Loss: 22288075.649\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 141, Loss: 22294097.722\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 142, Loss: 22298960.804\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 143, Loss: 22292904.742\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 144, Loss: 22309507.340\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 145, Loss: 22288655.907\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 146, Loss: 22289607.784\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 147, Loss: 22295160.794\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 148, Loss: 22304903.825\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 149, Loss: 22277792.216\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 150, Loss: 22283942.495\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 151, Loss: 22327271.077\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 152, Loss: 22351319.247\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 153, Loss: 22307012.433\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 154, Loss: 22314533.691\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 155, Loss: 22358380.979\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 156, Loss: 22292140.443\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 157, Loss: 22356219.541\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 158, Loss: 22272348.206\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 159, Loss: 22267177.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 160, Loss: 22297824.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 161, Loss: 22275640.289\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 162, Loss: 22285192.454\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 163, Loss: 22262516.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 164, Loss: 22328934.804\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 165, Loss: 22277090.918\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 166, Loss: 22326089.103\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 167, Loss: 22285784.299\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 168, Loss: 22257189.742\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 169, Loss: 22283107.619\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 170, Loss: 22301727.144\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 171, Loss: 22295145.835\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 172, Loss: 22286525.113\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 173, Loss: 22287968.412\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 174, Loss: 22318357.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 175, Loss: 22257522.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 176, Loss: 22277011.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 177, Loss: 22262949.639\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 178, Loss: 22300496.402\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 179, Loss: 22314082.320\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 180, Loss: 22277703.959\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 181, Loss: 22290049.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 182, Loss: 22273334.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 183, Loss: 22285945.619\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 184, Loss: 22283249.093\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 185, Loss: 22315176.433\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 186, Loss: 22273609.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 187, Loss: 22278040.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 188, Loss: 22306002.165\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 189, Loss: 22299471.165\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 190, Loss: 22279688.423\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 191, Loss: 22299137.062\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 192, Loss: 22317625.691\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 193, Loss: 22351516.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 194, Loss: 22295909.196\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 195, Loss: 22279387.392\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 196, Loss: 22302769.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 197, Loss: 22314243.856\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 198, Loss: 22272011.866\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 199, Loss: 22324321.423\n"
     ]
    }
   ],
   "source": [
    "dagmm = TrainerDAGMM(args, data, device)\n",
    "dagmm.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/DAGMM/IMG4.png\" height = \"60%\" width = \"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss function : compression 네트워크에서의 reconstruction error로 만들어진 error\n",
    "- E(z) : input 샘플을 관찰할 확률 (sample energy를 최소화하며 compression과 estimation 네트워크의 최상 조합을 찾을 수 있음)\n",
    "- lambda : DAGMM의 meta 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloaders, device, n_gmm):\n",
    "    ## DAGMM model test\n",
    "    dataloader_train, dataloader_test = dataloaders\n",
    "    model.eval()\n",
    "    print('Testing...')\n",
    "    compute = ComputeLoss(model, None, None, device, n_gmm)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        N_samples = 0\n",
    "        gamma_sum = 0\n",
    "        mu_sum = 0\n",
    "        cov_sum = 0\n",
    "    ## training 데이터를 이용하여 gamma, mu, cov 파라미터를 얻음\n",
    "        for x, _ in dataloader_train:\n",
    "            x = x.float().to(device)\n",
    "\n",
    "            _, _, z, gamma = model(x)\n",
    "            phi_batch, mu_batch, cov_batch = compute.compute_params(z, gamma)\n",
    "\n",
    "            batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "            gamma_sum += batch_gamma_sum\n",
    "            mu_sum += mu_batch * batch_gamma_sum.unsqueeze(-1)\n",
    "            cov_sum += cov_batch * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "            \n",
    "            N_samples += x.size(0)\n",
    "            \n",
    "        train_phi = gamma_sum / N_samples\n",
    "        train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n",
    "        train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # train 데이터를 위한 label, 에너지 score를 얻음\n",
    "        energy_train = []\n",
    "        labels_train = []\n",
    "\n",
    "        for x, y in dataloader_train:\n",
    "            x = x.float().to(device)\n",
    "\n",
    "            _, _, z, gamma = model(x)\n",
    "            sample_energy, cov_diag  = compute.compute_energy(z, gamma, phi=train_phi, mu=train_mu, cov=train_cov, sample_mean=False)\n",
    "            \n",
    "            energy_train.append(sample_energy.detach().cpu())\n",
    "            labels_train.append(y)\n",
    "        \n",
    "        energy_train = torch.cat(energy_train).numpy()\n",
    "        labels_train = torch.cat(labels_train).numpy()\n",
    "\n",
    "        # 테스트 데이터에 라벨과 에너지 score 얻음\n",
    "        energy_test = []\n",
    "        labels_test = []\n",
    "        for x, y in dataloader_test:\n",
    "            x = x.float().to(device)\n",
    "\n",
    "            _, _, z, gamma = model(x)\n",
    "            sample_energy, cov_diag  = compute.compute_energy(z, gamma, train_phi,train_mu, train_cov, sample_mean=False)\n",
    "            \n",
    "            energy_test.append(sample_energy.detach().cpu())\n",
    "            labels_test.append(y)\n",
    "            \n",
    "        energy_test = torch.cat(energy_test).numpy()\n",
    "        labels_test = torch.cat(labels_test).numpy()\n",
    "    \n",
    "        scores_total = np.concatenate((energy_train, energy_test), axis=0)\n",
    "        labels_total = np.concatenate((labels_train, labels_test), axis=0)\n",
    "\n",
    "    # energy값에 대해 threhold를 이용하여 이상치 탐지\n",
    "    threshold = np.percentile(scores_total, 100 - 20)\n",
    "    pred = (energy_test > threshold).astype(int)\n",
    "    gt = labels_test.astype(int)\n",
    "    precision, recall, f_score, _ = prf(gt, pred, average='binary')\n",
    "\n",
    "    \n",
    "    print(\"Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(precision, recall, f_score))\n",
    "    print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels_total, scores_total)*100))\n",
    "    return labels_total, scores_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Precision : 0.8427, Recall : 0.7385, F-score : 0.7872\n",
      "ROC AUC score: 96.71\n"
     ]
    }
   ],
   "source": [
    "labels, scores = eval(dagmm.model, data, device, args.n_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Precision|Recall|F1-Score|ROC-AUC|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|0.9561|0.9306|0.9432|99.11|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_in = scores[np.where(labels==0)[0]]\n",
    "scores_out = scores[np.where(labels==1)[0]]\n",
    "\n",
    "\n",
    "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
    "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers ')\n",
    "out_.plot.kde(ax=ax, legend=True)\n",
    "ax.grid(axis='x')\n",
    "ax.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **References**\n",
    "\n",
    "- Pytorch- DAGMM [link](https://github.com/mperezcarrasco/PyTorch-DAGMM.git)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
